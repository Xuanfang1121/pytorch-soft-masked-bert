[strings]
# Mode : train, test, serve
train_data_path = ./data/sighan_data/train_data_demo.xlsx
test_data_path = ./data/sighan_data/test_data_demo.xlsx
# train_data_path = ./data/toutiao_data/train_data.xlsx
# test_data_path = ./data/toutiao_data/test_data.xlsx

pretrain_model_path = D:/Spyder/pretrain_model/transformers_torch_tf/chinese-bert-wwm-ext/
vocab_path = D:/Spyder/pretrain_model/transformers_torch_tf/chinese-bert-wwm-ext/vocab.txt
# pretrain_model_path = /home/nlp/pretrain_model/bert-base-chinese/
# vocab_path = /home/nlp/pretrain_model/bert-base-chinese/vocab.txt
# pretrain_model_path = /opt/nlp/pretrain_model/chinese-bert-wwm-ext/
# vocab_path = /opt/nlp/pretrain_model/chinese-bert-wwm-ext/vocab.txt
pretrain_model_type = bert

# gpu ids
gpu_ids = -1
# save para
output_path = ./output/
model_name = model_sighan.pt

[ints]
hidden_size = 256
max_length = 64
embedding_size = 768
epoch = 300
batch_size = 2
pre_epoch_print = 50

[floats]
lr = 1e-5
gama = 0.8
mask_rate = 0.15

[bools]
data_mask = False